#kept as backup - for full yarn setup later
networks:
  spark-hive:
    driver: bridge

services:
  namenode:
    image: bde2020/hadoop-namenode:3.3.6
    container_name: namenode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_webhdfs_enabled=true
    ports:
      - "50070:50070"  # WebHDFS port
    networks:
      - spark-hive
    # Uncomment the following lines to enable resource limits
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '2.0'
    #       memory: '4G'

  datanode:
    image: bde2020/hadoop-datanode:3.3.6
    container_name: datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_webhdfs_enabled=true
    ports:
      - "50075:50075"  # WebHDFS port
    networks:
      - spark-hive
    # Uncomment the following lines to enable resource limits
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '2.0'
    #       memory: '4G'

  hive-metastore-postgresql:
    image: bde2020/hive-metastore-postgresql:2.3.0
    container_name: hive-metastore-postgresql
    environment:
      - POSTGRES_DB=metastore
      # - POSTGRES_USER=hive
      # - POSTGRES_PASSWORD=hivepassword
    networks:
      - spark-hive
    # Uncomment the following lines to enable resource limits
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '1.0'
    #       memory: '2G'

  hive-metastore:
    image: bde2020/hive-metastore:3.1.3
    container_name: hive-metastore
    depends_on:
      - namenode
      - datanode
      - hive-metastore-postgresql
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HIVE_METASTORE_DB_TYPE=postgres
      - HIVE_METASTORE_DB_HOST=hive-metastore-postgresql
      - HIVE_METASTORE_DB_PORT=5432
      - HIVE_METASTORE_DB_NAME=metastore
      # - HIVE_METASTORE_DB_USER=hive
      # - HIVE_METASTORE_DB_PASS=hivepassword
    ports:
      - "9083:9083"
    networks:
      - spark-hive
    # Uncomment the following lines to enable resource limits
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '1.0'
    #       memory: '2G'

  hive-server:
    image: bde2020/hive:3.1.3
    container_name: hive-server
    depends_on:
      - namenode
      - datanode
      - hive-metastore
    environment:
      - HIVE_CORE_CONF_javax_jdo_option_ConnectionURL=jdbc:postgresql://hive-metastore-postgresql:5432/metastore
      - SERVICE_PRECONDITION=hive-metastore:9083
    ports:
      - "10000:10000"
    networks:
      - spark-hive
    # Uncomment the following lines to enable resource limits
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '2.0'
    #       memory: '4G'

  spark-master:
    image: bitnami/spark:3.5.2
    container_name: spark-master
    depends_on:
      - namenode
      - datanode
    ports:
      - "8080:8080"  # Spark Master web UI
      - "7077:7077"  # Spark Master to Worker communication
    environment:
      - SPARK_MODE=master,worker  # Configure master to also act as a worker
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    networks:
      - spark-hive
    # Uncomment the following lines to enable resource limits
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '2.0'
    #       memory: '4G'

  spark-worker-1:
    image: bitnami/spark:3.5.2
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"  # Spark Worker web UI
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    networks:
      - spark-hive
    # Uncomment the following lines to enable resource limits
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '1.0'
    #       memory: '2G'

  # Uncomment the following section to add another Spark Worker
  # spark-worker-2:
  #   image: bitnami/spark:3.5.2
  #   container_name: spark-worker-2
  #   depends_on:
  #     - spark-master
  #   ports:
  #     - "8082:8081"  # Spark Worker web UI
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER_URL=spark://spark-master:7077
  #     - SPARK_WORKER_MEMORY=1G
  #     - SPARK_WORKER_CORES=1
  #     - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #     - SPARK_RPC_ENCRYPTION_ENABLED=no
  #     - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #     - SPARK_SSL_ENABLED=no
  #     - SPARK_USER=spark
  #     - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
  #   networks:
  #     - spark-hive
  #   # Uncomment the following lines to enable resource limits
  #   # deploy:
  #   #   resources:
  #     limits:
  #       cpus: '1.0'
  #       memory: '2G'

#  resourcemanager:
#    image: bde2020/hadoop-resourcemanager:3.3.6
#    container_name: resourcemanager
#    depends_on:
#      - namenode
#      - datanode
#    environment:
#      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
#    ports:
#      - "8088:8088"  # ResourceManager web UI
#    networks:
#      - spark-hive
#    # Uncomment the following lines to enable resource limits
#    # deploy:
#    #   resources:
#    #     limits:
#    #       cpus: '2.0'
#    #       memory: '4G'

#  nodemanager:
#    image: bde2020/hadoop-nodemanager:3.3.6
#    container_name: nodemanager
#    depends_on:
#      - resourcemanager
#      - namenode
#      - datanode
#    environment:
#      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
#    ports:
#      - "8042:8042"  # NodeManager web UI
#    networks:
#      - spark-hive
#    # Uncomment the following lines to enable resource limits
#    # deploy:
#    #   resources:
#    #     limits:
#    #       cpus: '2.0'
#    #       memory: '4G'

#  historyserver:
#    image: bde2020/hadoop-historyserver:3.3.6
#    container_name: historyserver
#    depends_on:
#      - resourcemanager
#      - namenode
#      - datanode
#    environment:
#      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
#    ports:
#      - "8188:8188"  # HistoryServer web UI
#    networks:
#      - spark-hive
#    # Uncomment the following lines to enable resource limits
#    # deploy:
#    #   resources:
#    #     limits:
#    #       cpus: '2.0'
#    #       memory: '4G'

